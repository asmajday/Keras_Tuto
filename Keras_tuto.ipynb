{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Keras_tuto.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"E8N6fSyOLZhe","colab_type":"code","colab":{}},"source":["from numpy import loadtxt\n","from keras.models import Sequential\n","from keras.layers import Dense"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"o64vReTLLq2Q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"3d57fc54-2e7a-4e04-a498-a7c440ba7cf5","executionInfo":{"status":"ok","timestamp":1576004532572,"user_tz":-60,"elapsed":16351,"user":{"displayName":"asma jday","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCoiARXXTroeYDfidCrEGW5BXoaxZuHUDLoHMAUVQ=s64","userId":"01373681922022568072"}}},"source":["# load the dataset\n","dataset = loadtxt('pima-indians-diabetes.csv', delimiter=',')\n","# split into input (X) and output (y) variables\n","X = dataset[:,0:8]\n","y = dataset[:,8]\n","# define the keras model\n","model = Sequential()\n","model.add(Dense(12, input_dim=8, activation='relu'))\n","model.add(Dense(8, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","# compile the keras model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","# fit the keras model on the dataset\n","model.fit(X, y, epochs=150, batch_size=10)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","Epoch 1/150\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","768/768 [==============================] - 1s 1ms/step - loss: 3.6714 - acc: 0.6484\n","Epoch 2/150\n","768/768 [==============================] - 0s 97us/step - loss: 3.1743 - acc: 0.6328\n","Epoch 3/150\n","768/768 [==============================] - 0s 111us/step - loss: 3.1092 - acc: 0.6315\n","Epoch 4/150\n","768/768 [==============================] - 0s 122us/step - loss: 3.0775 - acc: 0.6315\n","Epoch 5/150\n","768/768 [==============================] - 0s 117us/step - loss: 3.0437 - acc: 0.6497\n","Epoch 6/150\n","768/768 [==============================] - 0s 109us/step - loss: 2.9637 - acc: 0.6445\n","Epoch 7/150\n","768/768 [==============================] - 0s 132us/step - loss: 1.8329 - acc: 0.5924\n","Epoch 8/150\n","768/768 [==============================] - 0s 149us/step - loss: 0.9851 - acc: 0.5794\n","Epoch 9/150\n","768/768 [==============================] - 0s 126us/step - loss: 0.7824 - acc: 0.6523\n","Epoch 10/150\n","768/768 [==============================] - 0s 123us/step - loss: 0.7193 - acc: 0.6536\n","Epoch 11/150\n","768/768 [==============================] - 0s 131us/step - loss: 0.6883 - acc: 0.6719\n","Epoch 12/150\n","768/768 [==============================] - 0s 123us/step - loss: 0.6589 - acc: 0.6745\n","Epoch 13/150\n","768/768 [==============================] - 0s 114us/step - loss: 0.6584 - acc: 0.6823\n","Epoch 14/150\n","768/768 [==============================] - 0s 111us/step - loss: 0.6481 - acc: 0.6849\n","Epoch 15/150\n","768/768 [==============================] - 0s 128us/step - loss: 0.6392 - acc: 0.6953\n","Epoch 16/150\n","768/768 [==============================] - 0s 131us/step - loss: 0.6372 - acc: 0.6849\n","Epoch 17/150\n","768/768 [==============================] - 0s 134us/step - loss: 0.6442 - acc: 0.6758\n","Epoch 18/150\n","768/768 [==============================] - 0s 147us/step - loss: 0.6314 - acc: 0.6797\n","Epoch 19/150\n","768/768 [==============================] - 0s 159us/step - loss: 0.6324 - acc: 0.6875\n","Epoch 20/150\n","768/768 [==============================] - 0s 128us/step - loss: 0.6305 - acc: 0.6784\n","Epoch 21/150\n","768/768 [==============================] - 0s 122us/step - loss: 0.6425 - acc: 0.6784\n","Epoch 22/150\n","768/768 [==============================] - 0s 105us/step - loss: 0.6293 - acc: 0.6862\n","Epoch 23/150\n","768/768 [==============================] - 0s 108us/step - loss: 0.6342 - acc: 0.6784\n","Epoch 24/150\n","768/768 [==============================] - 0s 130us/step - loss: 0.6287 - acc: 0.6823\n","Epoch 25/150\n","768/768 [==============================] - 0s 115us/step - loss: 0.6344 - acc: 0.6758\n","Epoch 26/150\n","768/768 [==============================] - 0s 104us/step - loss: 0.6306 - acc: 0.6849\n","Epoch 27/150\n","768/768 [==============================] - 0s 110us/step - loss: 0.6262 - acc: 0.6810\n","Epoch 28/150\n","768/768 [==============================] - 0s 114us/step - loss: 0.6206 - acc: 0.6940\n","Epoch 29/150\n","768/768 [==============================] - 0s 124us/step - loss: 0.6183 - acc: 0.6862\n","Epoch 30/150\n","768/768 [==============================] - 0s 125us/step - loss: 0.6182 - acc: 0.6927\n","Epoch 31/150\n","768/768 [==============================] - 0s 134us/step - loss: 0.6188 - acc: 0.6901\n","Epoch 32/150\n","768/768 [==============================] - 0s 109us/step - loss: 0.6222 - acc: 0.6875\n","Epoch 33/150\n","768/768 [==============================] - 0s 116us/step - loss: 0.6236 - acc: 0.6901\n","Epoch 34/150\n","768/768 [==============================] - 0s 109us/step - loss: 0.6195 - acc: 0.6940\n","Epoch 35/150\n","768/768 [==============================] - 0s 107us/step - loss: 0.6146 - acc: 0.6901\n","Epoch 36/150\n","768/768 [==============================] - 0s 119us/step - loss: 0.6137 - acc: 0.6927\n","Epoch 37/150\n","768/768 [==============================] - 0s 116us/step - loss: 0.6137 - acc: 0.6966\n","Epoch 38/150\n","768/768 [==============================] - 0s 123us/step - loss: 0.6210 - acc: 0.6888\n","Epoch 39/150\n","768/768 [==============================] - 0s 111us/step - loss: 0.6163 - acc: 0.6849\n","Epoch 40/150\n","768/768 [==============================] - 0s 121us/step - loss: 0.6227 - acc: 0.6966\n","Epoch 41/150\n","768/768 [==============================] - 0s 126us/step - loss: 0.6116 - acc: 0.7018\n","Epoch 42/150\n","768/768 [==============================] - 0s 124us/step - loss: 0.6145 - acc: 0.6875\n","Epoch 43/150\n","768/768 [==============================] - 0s 128us/step - loss: 0.6102 - acc: 0.6940\n","Epoch 44/150\n","768/768 [==============================] - 0s 119us/step - loss: 0.6070 - acc: 0.7005\n","Epoch 45/150\n","768/768 [==============================] - 0s 111us/step - loss: 0.6108 - acc: 0.6953\n","Epoch 46/150\n","768/768 [==============================] - 0s 115us/step - loss: 0.6040 - acc: 0.6966\n","Epoch 47/150\n","768/768 [==============================] - 0s 120us/step - loss: 0.6130 - acc: 0.6914\n","Epoch 48/150\n","768/768 [==============================] - 0s 121us/step - loss: 0.6120 - acc: 0.6862\n","Epoch 49/150\n","768/768 [==============================] - 0s 126us/step - loss: 0.6098 - acc: 0.6966\n","Epoch 50/150\n","768/768 [==============================] - 0s 124us/step - loss: 0.6089 - acc: 0.6966\n","Epoch 51/150\n","768/768 [==============================] - 0s 134us/step - loss: 0.6084 - acc: 0.6953\n","Epoch 52/150\n","768/768 [==============================] - 0s 134us/step - loss: 0.6127 - acc: 0.6927\n","Epoch 53/150\n","768/768 [==============================] - 0s 131us/step - loss: 0.6081 - acc: 0.6966\n","Epoch 54/150\n","768/768 [==============================] - 0s 142us/step - loss: 0.6111 - acc: 0.6927\n","Epoch 55/150\n","768/768 [==============================] - 0s 128us/step - loss: 0.6161 - acc: 0.6836\n","Epoch 56/150\n","768/768 [==============================] - 0s 115us/step - loss: 0.6058 - acc: 0.6979\n","Epoch 57/150\n","768/768 [==============================] - 0s 125us/step - loss: 0.6110 - acc: 0.6914\n","Epoch 58/150\n","768/768 [==============================] - 0s 124us/step - loss: 0.6225 - acc: 0.6888\n","Epoch 59/150\n","768/768 [==============================] - 0s 134us/step - loss: 0.6059 - acc: 0.6979\n","Epoch 60/150\n","768/768 [==============================] - 0s 118us/step - loss: 0.6098 - acc: 0.6966\n","Epoch 61/150\n","768/768 [==============================] - 0s 113us/step - loss: 0.6076 - acc: 0.6901\n","Epoch 62/150\n","768/768 [==============================] - 0s 135us/step - loss: 0.6096 - acc: 0.7005\n","Epoch 63/150\n","768/768 [==============================] - 0s 131us/step - loss: 0.6075 - acc: 0.6914\n","Epoch 64/150\n","768/768 [==============================] - 0s 127us/step - loss: 0.6107 - acc: 0.6966\n","Epoch 65/150\n","768/768 [==============================] - 0s 122us/step - loss: 0.6080 - acc: 0.7018\n","Epoch 66/150\n","768/768 [==============================] - 0s 122us/step - loss: 0.6020 - acc: 0.7044\n","Epoch 67/150\n","768/768 [==============================] - 0s 133us/step - loss: 0.6134 - acc: 0.6979\n","Epoch 68/150\n","768/768 [==============================] - 0s 117us/step - loss: 0.6099 - acc: 0.6875\n","Epoch 69/150\n","768/768 [==============================] - 0s 116us/step - loss: 0.6017 - acc: 0.7018\n","Epoch 70/150\n","768/768 [==============================] - 0s 112us/step - loss: 0.6066 - acc: 0.6966\n","Epoch 71/150\n","768/768 [==============================] - 0s 111us/step - loss: 0.6016 - acc: 0.7005\n","Epoch 72/150\n","768/768 [==============================] - 0s 109us/step - loss: 0.6030 - acc: 0.7031\n","Epoch 73/150\n","768/768 [==============================] - 0s 132us/step - loss: 0.6025 - acc: 0.6953\n","Epoch 74/150\n","768/768 [==============================] - 0s 127us/step - loss: 0.6014 - acc: 0.6992\n","Epoch 75/150\n","768/768 [==============================] - 0s 128us/step - loss: 0.5988 - acc: 0.7096\n","Epoch 76/150\n","768/768 [==============================] - 0s 121us/step - loss: 0.6056 - acc: 0.7018\n","Epoch 77/150\n","768/768 [==============================] - 0s 122us/step - loss: 0.6066 - acc: 0.6940\n","Epoch 78/150\n","768/768 [==============================] - 0s 120us/step - loss: 0.6027 - acc: 0.6927\n","Epoch 79/150\n","768/768 [==============================] - 0s 137us/step - loss: 0.6005 - acc: 0.6979\n","Epoch 80/150\n","768/768 [==============================] - 0s 128us/step - loss: 0.6089 - acc: 0.7018\n","Epoch 81/150\n","768/768 [==============================] - 0s 127us/step - loss: 0.6001 - acc: 0.7044\n","Epoch 82/150\n","768/768 [==============================] - 0s 132us/step - loss: 0.6036 - acc: 0.7057\n","Epoch 83/150\n","768/768 [==============================] - 0s 137us/step - loss: 0.5958 - acc: 0.7083\n","Epoch 84/150\n","768/768 [==============================] - 0s 134us/step - loss: 0.5998 - acc: 0.7044\n","Epoch 85/150\n","768/768 [==============================] - 0s 117us/step - loss: 0.6015 - acc: 0.6875\n","Epoch 86/150\n","768/768 [==============================] - 0s 108us/step - loss: 0.6025 - acc: 0.6888\n","Epoch 87/150\n","768/768 [==============================] - 0s 109us/step - loss: 0.6124 - acc: 0.6927\n","Epoch 88/150\n","768/768 [==============================] - 0s 106us/step - loss: 0.6007 - acc: 0.6992\n","Epoch 89/150\n","768/768 [==============================] - 0s 108us/step - loss: 0.6040 - acc: 0.6953\n","Epoch 90/150\n","768/768 [==============================] - 0s 115us/step - loss: 0.5974 - acc: 0.7005\n","Epoch 91/150\n","768/768 [==============================] - 0s 111us/step - loss: 0.5956 - acc: 0.6966\n","Epoch 92/150\n","768/768 [==============================] - 0s 112us/step - loss: 0.5945 - acc: 0.7005\n","Epoch 93/150\n","768/768 [==============================] - 0s 113us/step - loss: 0.5995 - acc: 0.7005\n","Epoch 94/150\n","768/768 [==============================] - 0s 113us/step - loss: 0.5926 - acc: 0.6979\n","Epoch 95/150\n","768/768 [==============================] - 0s 124us/step - loss: 0.5997 - acc: 0.6927\n","Epoch 96/150\n","768/768 [==============================] - 0s 120us/step - loss: 0.5936 - acc: 0.7005\n","Epoch 97/150\n","768/768 [==============================] - 0s 114us/step - loss: 0.5928 - acc: 0.7057\n","Epoch 98/150\n","768/768 [==============================] - 0s 107us/step - loss: 0.5922 - acc: 0.7018\n","Epoch 99/150\n","768/768 [==============================] - 0s 107us/step - loss: 0.5836 - acc: 0.7057\n","Epoch 100/150\n","768/768 [==============================] - 0s 109us/step - loss: 0.5918 - acc: 0.6992\n","Epoch 101/150\n","768/768 [==============================] - 0s 109us/step - loss: 0.5887 - acc: 0.6992\n","Epoch 102/150\n","768/768 [==============================] - 0s 118us/step - loss: 0.5948 - acc: 0.6992\n","Epoch 103/150\n","768/768 [==============================] - 0s 110us/step - loss: 0.5967 - acc: 0.6966\n","Epoch 104/150\n","768/768 [==============================] - 0s 111us/step - loss: 0.5884 - acc: 0.7044\n","Epoch 105/150\n","768/768 [==============================] - 0s 106us/step - loss: 0.5927 - acc: 0.7083\n","Epoch 106/150\n","768/768 [==============================] - 0s 109us/step - loss: 0.5937 - acc: 0.7044\n","Epoch 107/150\n","768/768 [==============================] - 0s 121us/step - loss: 0.6018 - acc: 0.6953\n","Epoch 108/150\n","768/768 [==============================] - 0s 123us/step - loss: 0.5879 - acc: 0.7031\n","Epoch 109/150\n","768/768 [==============================] - 0s 118us/step - loss: 0.5833 - acc: 0.7122\n","Epoch 110/150\n","768/768 [==============================] - 0s 112us/step - loss: 0.5849 - acc: 0.7031\n","Epoch 111/150\n","768/768 [==============================] - 0s 110us/step - loss: 0.5954 - acc: 0.6966\n","Epoch 112/150\n","768/768 [==============================] - 0s 115us/step - loss: 0.5931 - acc: 0.7005\n","Epoch 113/150\n","768/768 [==============================] - 0s 111us/step - loss: 0.5853 - acc: 0.6953\n","Epoch 114/150\n","768/768 [==============================] - 0s 113us/step - loss: 0.5882 - acc: 0.7031\n","Epoch 115/150\n","768/768 [==============================] - 0s 110us/step - loss: 0.5856 - acc: 0.7031\n","Epoch 116/150\n","768/768 [==============================] - 0s 107us/step - loss: 0.5855 - acc: 0.7005\n","Epoch 117/150\n","768/768 [==============================] - 0s 117us/step - loss: 0.5873 - acc: 0.6966\n","Epoch 118/150\n","768/768 [==============================] - 0s 121us/step - loss: 0.5820 - acc: 0.7057\n","Epoch 119/150\n","768/768 [==============================] - 0s 118us/step - loss: 0.5971 - acc: 0.6940\n","Epoch 120/150\n","768/768 [==============================] - 0s 115us/step - loss: 0.5835 - acc: 0.7070\n","Epoch 121/150\n","768/768 [==============================] - 0s 109us/step - loss: 0.5889 - acc: 0.6979\n","Epoch 122/150\n","768/768 [==============================] - 0s 114us/step - loss: 0.5879 - acc: 0.7044\n","Epoch 123/150\n","768/768 [==============================] - 0s 110us/step - loss: 0.5908 - acc: 0.7044\n","Epoch 124/150\n","768/768 [==============================] - 0s 109us/step - loss: 0.5827 - acc: 0.7044\n","Epoch 125/150\n","768/768 [==============================] - 0s 125us/step - loss: 0.5803 - acc: 0.7109\n","Epoch 126/150\n","768/768 [==============================] - 0s 146us/step - loss: 0.5855 - acc: 0.7083\n","Epoch 127/150\n","768/768 [==============================] - 0s 128us/step - loss: 0.5809 - acc: 0.7018\n","Epoch 128/150\n","768/768 [==============================] - 0s 117us/step - loss: 0.5830 - acc: 0.6914\n","Epoch 129/150\n","768/768 [==============================] - 0s 123us/step - loss: 0.5778 - acc: 0.6992\n","Epoch 130/150\n","768/768 [==============================] - 0s 124us/step - loss: 0.5875 - acc: 0.6992\n","Epoch 131/150\n","768/768 [==============================] - 0s 129us/step - loss: 0.5786 - acc: 0.6940\n","Epoch 132/150\n","768/768 [==============================] - 0s 133us/step - loss: 0.5838 - acc: 0.7044\n","Epoch 133/150\n","768/768 [==============================] - 0s 128us/step - loss: 0.5797 - acc: 0.7083\n","Epoch 134/150\n","768/768 [==============================] - 0s 131us/step - loss: 0.5820 - acc: 0.6966\n","Epoch 135/150\n","768/768 [==============================] - 0s 129us/step - loss: 0.5795 - acc: 0.7031\n","Epoch 136/150\n","768/768 [==============================] - 0s 135us/step - loss: 0.5807 - acc: 0.7109\n","Epoch 137/150\n","768/768 [==============================] - 0s 153us/step - loss: 0.5737 - acc: 0.7018\n","Epoch 138/150\n","768/768 [==============================] - 0s 110us/step - loss: 0.5833 - acc: 0.7109\n","Epoch 139/150\n","768/768 [==============================] - 0s 108us/step - loss: 0.5738 - acc: 0.7057\n","Epoch 140/150\n","768/768 [==============================] - 0s 112us/step - loss: 0.5781 - acc: 0.7057\n","Epoch 141/150\n","768/768 [==============================] - 0s 141us/step - loss: 0.5712 - acc: 0.7005\n","Epoch 142/150\n","768/768 [==============================] - 0s 130us/step - loss: 0.5742 - acc: 0.7070\n","Epoch 143/150\n","768/768 [==============================] - 0s 126us/step - loss: 0.5715 - acc: 0.7109\n","Epoch 144/150\n","768/768 [==============================] - 0s 146us/step - loss: 0.5692 - acc: 0.7122\n","Epoch 145/150\n","768/768 [==============================] - 0s 131us/step - loss: 0.5756 - acc: 0.7018\n","Epoch 146/150\n","768/768 [==============================] - 0s 130us/step - loss: 0.5772 - acc: 0.6927\n","Epoch 147/150\n","768/768 [==============================] - 0s 129us/step - loss: 0.5771 - acc: 0.6940\n","Epoch 148/150\n","768/768 [==============================] - 0s 113us/step - loss: 0.5718 - acc: 0.7005\n","Epoch 149/150\n","768/768 [==============================] - 0s 113us/step - loss: 0.5762 - acc: 0.7057\n","Epoch 150/150\n","768/768 [==============================] - 0s 111us/step - loss: 0.5703 - acc: 0.7070\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f31bbd35860>"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"o__oevoxLzVz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"bd1c2b05-6f30-48b8-f528-cb9f2e7784e9","executionInfo":{"status":"ok","timestamp":1576004542269,"user_tz":-60,"elapsed":903,"user":{"displayName":"asma jday","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCoiARXXTroeYDfidCrEGW5BXoaxZuHUDLoHMAUVQ=s64","userId":"01373681922022568072"}}},"source":["# evaluate the keras model\n","_, accuracy = model.evaluate(X, y)\n","print('Accuracy: %.2f' % (accuracy*100))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["768/768 [==============================] - 0s 63us/step\n","Accuracy: 70.83\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1RtSMEjkL3QP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"outputId":"75a8cc61-86b7-4e93-ae5a-9742d28ed13e","executionInfo":{"status":"ok","timestamp":1576004546915,"user_tz":-60,"elapsed":1167,"user":{"displayName":"asma jday","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCoiARXXTroeYDfidCrEGW5BXoaxZuHUDLoHMAUVQ=s64","userId":"01373681922022568072"}}},"source":["# make class predictions with the model\n","predictions = model.predict_classes(X)\n","# summarize the first 5 cases\n","for i in range(5):\n","\tprint('%s => %d (expected %d)' % (X[i].tolist(), predictions[i], y[i]))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["[6.0, 148.0, 72.0, 35.0, 0.0, 33.6, 0.627, 50.0] => 1 (expected 1)\n","[1.0, 85.0, 66.0, 29.0, 0.0, 26.6, 0.351, 31.0] => 0 (expected 0)\n","[8.0, 183.0, 64.0, 0.0, 0.0, 23.3, 0.672, 32.0] => 1 (expected 1)\n","[1.0, 89.0, 66.0, 23.0, 94.0, 28.1, 0.167, 21.0] => 0 (expected 0)\n","[0.0, 137.0, 40.0, 35.0, 168.0, 43.1, 2.288, 33.0] => 0 (expected 1)\n"],"name":"stdout"}]}]}